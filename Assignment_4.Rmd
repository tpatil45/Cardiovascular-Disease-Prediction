---
title: "Assignement_4"
author: "Tejas Patil"
date: "2020 M04 21"
output: html_document
---

#### __Executive Summary__:
##### __Problem Statement:__
##### Disease dataset provides the information of people and their physical and habitual attributes that can be helpful in predicting if that person will suffer from a particular type of disease or not. The aim of this project is to build the data analytical models that will help in predicting a person as disease prone or disease safe.
##### __1. Data Load and Cleanup:__
##### This section deals with cleaning the data. We'll check the data for missing values, outliers etc. and treat those observations with suitable values. The data will also be checked for presence of any duplicate values. It also includes the statistically analysing the data.
##### __2. EDA and Feature selection:__
##### This section involves a deeper understanding of the data using tabular and visual representation of the data and __creating new columns__ (tranforming data into a useful form). We'll use __logistic regression__ for the feature selection. Note that we are using logistic regression here because this a binary classification problem and logistic regression can provide good variable importance.
##### __3.Model building and evaluation:__
##### In this section, we will buil 6 different classification models namely: K-Nearest neighbors, Naive Bayes classifier, Random Forest, Supply vector machine linear and non linear, and finally Gradient boosting machine model. Models will be evaluated based on suitable evaluation parameters __(Accuracy and Recall)__. At last, we will use these models for predictions on test dataset.

### __1. Data load and Cleanup__

##### Importing the necessary libraries 
```{r}
library(ggplot2)
library(gridExtra)
library(caret)
#library(mlbench)
#library(MLmetrics)
library(fastDummies)
#library(data.table)
library(corrplot)
library(pROC)
#library(klaR)
library(kernlab)
library(gbm)
library(glmnet)
library(e1071)
library(tidyverse)
library(keras)
library(yardstick)
library(rsample)
library(recipes)
library(hrbrthemes)
library(viridis)
library(C50)
library(rpart)

setwd('C:/Users/hp/Desktop/IST 707/Assignments/Assignment 4')
#getwd()
```

##### Importing the csv file into R enviornment and checking the structure and statistical summary of the dataset
##### Some early observations from the data:
##### AGE: minimum age is 29 years and maximum is 64 years. Hence our population for this problem is mainly adult people and people in early stages of old age
##### GENDER: training data is pretty imbalanced with respect to the gender variable. Females are almost twice that of male population.
##### Smoke, Alcohol and Excercise are classified as 1 and 0. The variable importance of these variables is doubtful as some people might be doing one of these things in small/large quantity relative to the other quantity.
```{r}
train_df <- read.csv('Disease Prediction Training.csv', stringsAsFactors = T)
#summary(train_df)
#str(train_df)
```

##### Checked the data for Missing values. The results returned shows us that there are __no missing values__ in the data
```{r}
apply(is.na(train_df),2,sum)
```

##### Checked the data for duplicate observations. The results returned shows us that there were __1752 duplicate observations__ in the data. So, we have selected only the unique values
```{r}
nrow(train_df) - nrow(unique(train_df))
train_un <- unique(train_df)
```

### __2. Exploratory Data Analysis__

#### __Analysing the target variable__
##### We find that target variable is of binary type and has 2 levels in it. So, this will be a binary classification problem. Here most important observation is that our target variable is balanced
```{r}
str(train_un$Disease)
table(train_un$Disease)
```

##### For the further EDA, we create 2 new dataframes containing only categorical and only numerical attributes respectively. Before that we convert all binary integer variables to factor for the easying the analysis process
```{r}
train_un$Smoke <- as.factor(train_un$Smoke)
train_un$Alcohol <- as.factor(train_un$Alcohol)
train_un$Exercise <- as.factor(train_un$Exercise)
train_un$Disease <- as.factor(train_un$Disease)


cat_df <- train_un[sapply(train_un, is.character) | sapply(train_un, is.factor)]
num_df <- train_un[sapply(train_un, is.numeric) | sapply(train_un, is.integer)]
```

##### Creating a function to generate bar plot for all the categorical variables and then using the grid plot to display all the visualization.
##### Impotant observations here are that most of the people do not smoke or drink and do excercise regularly. Ideally, the disease variable should have more '0's than '1's which questions the variable impotance for these three variables. 
```{r}
bar_plot <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + 
    theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}
mygrid1 <- list()
for (i in 1:length(cat_df)){
  myplot1 <- bar_plot(cat_df, i)
  mygrid1 <- c(mygrid1, list(myplot1)) 
}
do.call("grid.arrange", c(mygrid1, ncol=2))
```

##### Again creating a function, this time to generate a grid plot for displaying boxplots for all the numerical variables. We see that there are considerable __outliers__ in variables named 'Height', 'Weight', 'High.Blood.Pressure' and 'Low.Blood.Pressure'.
```{r}
box_plot <- function(data_in, i) {
  p <- ggplot(data_in, aes(y = data_in[,i])) + geom_boxplot(color="black", fill="green", alpha=0.2) + ggtitle(colnames(data_in)[i]) + ylab("Distribution") + theme(legend.position="none")
  return (p)
}

mygrid2 <- list()
for (i in 1:length(num_df)){
  myplot2 <- box_plot(num_df, i)
  mygrid2 <- c(mygrid2, list(myplot2)) 
}
do.call("grid.arrange", c(mygrid2, ncol=2))
```



##### Now let us check the range of these outliters. We create a new dataframe and check the values for extreme percentile values from both the ends for variables containing outliers
```{r}
quantiles <- c(0.0, 0.001, 0.002, 0.003, 0.01, 0.985, 0.99, 0.997, 0.998, 0.999, 1)
out_df <- data.frame(row.names = quantiles)
for (i in 1:length(num_df)){
  q <- as.vector(quantile(num_df[,i], quantiles))
  out_df[,i] <- q
}
colnames(out_df) <- colnames(num_df)
out_df <- t(out_df) #taking the transpose
print(out_df)
```
##### Important observations from this dataframe:
##### Age: There are no outliers for age column, all values seem to be genuine
##### Height: The upper bound for the height variable seems to be ok, the lower bound for height variable is very extremely small. The shortest person in this data has a height of 55 cm. Considering the fact that the youngest person is of 29 years, we winsoroze this end of height variable to 0.001 percentile.
##### Weight: Again for weight variable, the upper bound seems to be fine. The lower bound has values of 10 kgs. A normal person who is atleast 29 years old, weighs more than that. Based on the table, we decide the keep the minimum weight of that person to be 40kgs
##### High.Blood.Pressure: High blood pressure which is also known as Systolic pressure is considered normal when the value is below 120 mm Hg but closer to 120. The minimum value in this column are very extreme and cannot be considered. Hence, we consider the minimum value to be 70 which is 0.003 percentile value. Also, we need to winsorize the upper limit here, because the the upper limit is shown to be 14020 and that is impossible. Any value above 180 is considered to be dangerous. So, we replce it with 220 which is the 0.999th percentile.
##### Low.Blood.Pressure: Low blood pressure is also known as Diastolic presssure. The minimum value for this data is 0, which is practically impossible for a living person. Any value less than but close to 80 is considered normal. Hence to maintain the distinguishibility we winsorize it to the 0.002th percentile. The upper bound, more than 500 values 1000 or 1100 or more than that, which is again impossible. So we winsorize it to the 0.985th percentile. 


##### Creating a function to treat outliers by winsorizing to specific boundaries and using it to treat all numerical variables (except age) to treat outliers
```{r}
Out_Treat_W = function(x, lb, ub){
  LC = quantile(x, lb)
  UC = quantile(x, ub)
  UOut <- which(x > UC)
  LOut <- which(x < LC)
  for (i in 1:length(UOut)){
    x[UOut[i]] <- UC
  }
  for (i in 1:length(LOut)){
    x[LOut[i]] <- LC
  }
  return(x)
}
train_un$Height <- Out_Treat_W(train_un$Height, 0.001, 1)
train_un$Weight <- Out_Treat_W(train_un$Weight, 0.001, 1)
train_un$Low.Blood.Pressure <- Out_Treat_W(train_un$Low.Blood.Pressure, 0.001, 0.985)
train_un$High.Blood.Pressure <- Out_Treat_W(train_un$High.Blood.Pressure, 0.003, 0.999)
```

##### Updating the num_data with winsorized numerical variable. Creating a function to generate Histogram for numerical variables. Implementing that function on every variable in num_df dataset. We observe that all the distribution are  slightly skewed but more or less normally distributed.
```{r}
num_df <- train_un[sapply(train_un, is.numeric) | sapply(train_un, is.integer)]

hist_plot <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(data[,1])) + 
    geom_histogram(bins = 30, col="red", aes(fill=..count..)) +
    xlab(colnames(data_in)[i]) +
    scale_fill_gradient("Count", low="green", high="red")
  return (p)
}
mygrid2 <- list()
for (i in 1:length(num_df)){
  myplot2 <- hist_plot(num_df, i)
  mygrid2 <- c(mygrid2, list(myplot2)) 
}
do.call("grid.arrange", c(mygrid2, ncol=2))
```

##### Plotting the correlation matrix to check for the multicollinearity between numeric independent variables. Only significant correlation is between __Low blood pressure__ and __High blod pressure__ variables.
```{r}
corMatrix <- cor(num_df)
#View(descrCor)
#print(descrCor)
corrplot(corMatrix)
```

## Bivariate Analysis
##### Making a function to plot boxplots for bivariate analysis
```{r}
bivariate_plot <- function(num_var, title){
  plt <- train_un %>% ggplot( aes(x=Disease, y=num_var, fill=Disease)) + geom_boxplot() +
    scale_fill_viridis(discrete = T, alpha=0.5) +
    theme(legend.position="none", plot.title = element_text(size=11)) +
    ggtitle(title) + xlab("")
  return(plt)
}
```

##### Low Blood Pressure vs Disease: This graph tells us that on an average low blood pressure was higher for the people wtih disease, which logically makes sense if the disease is related to heart.
```{r}
bivariate_plot(train_un$Low.Blood.Pressure, "Low BP Vs Disease")
```

##### High Blood Pressure vs Disease: Similar to Low BP, even high blood pressure has higher average values for people with disease rather than people that do not have the disease. 
```{r}
bivariate_plot(train_un$High.Blood.Pressure, "Weight Vs Disease")
```


#### Creating new variables:

##### We have two variables that measure the blood pressure (High and Low). Now, we will create a single categorical variable that will try to capture the combined effect of both variables on a person's health. We will the classifiy the health and unhealthy blood pressure ranges into 5 different categories namely: __Normal__, __Elevated__ (Slight increase), __Hypertension (Stage 1)__, __Hypertension (Stage 2)__ and __Hypertensive Crisis__ (Critical condition). These ranges are obtained using the domain knowledge from the website:
##### [https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings]

##### Checking if the variable can make a impact on our model
##### We observe that the number of people having the disease is very high for people of class Hypertension (Stage 1), Hypertension (Stage 2) and Hypertensive Crisis (Critical condition). This gives us __two conclusions__, 1] the newly created variable can make a big impact on our model and 2] There's very high probability that the disease we are studying, might be related to __cardiac failure__.  
```{r}
for (i in 1:nrow(train_un)) {
  if (train_un$High.Blood.Pressure[i] >= 180 | train_un$Low.Blood.Pressure[i] >= 120) {
    train_un$BP_Category[i] = "Hypertensive"
  }
  else if (train_un$High.Blood.Pressure[i] >= 140 | train_un$Low.Blood.Pressure[i] >= 90) {
    train_un$BP_Category[i] = "HT_stage2"
  }
  else if (train_un$High.Blood.Pressure[i] >= 130 | train_un$Low.Blood.Pressure[i] >= 80) {
    train_un$BP_Category[i] = "HT_stage1"
  }
  else if (train_un$High.Blood.Pressure[i] >= 120 & train_un$Low.Blood.Pressure[i] < 80) {
    train_un$BP_Category[i] = "Elevated"
  }
  else {
    train_un$BP_Category[i] = "Normal"
  }
}
train_un$BP_Category <- as.factor(train_un$BP_Category)
table(train_un$Disease, train_un$BP_Category)
```



##### Now we'll be creating two more new variable called BMI (Body Mass Index). BMI is a measure of body fat considering height and weight of a person, it applies to adult men and women. The second variable is nothing but the categorization of the BMI variable.
```{r}
for (i in 1:nrow(train_un)) {
  train_un$BMI[i] <- (train_un$Weight[i] / ((train_un$Height[i]/100)**2))
  if (train_un$BMI[i] <= 18) {
    train_un$BMI_CAT[i] = "Underweight"
  }
  else if (train_un$BMI[i] <= 24) {
    train_un$BMI_CAT[i] = "Healthy"
  }
  else if (train_un$BMI[i] <= 29) {
    train_un$BMI_CAT[i] = "Overweight"
  }
  else {
   train_un$BMI_CAT[i] = "Obese"
  }
}
train_un$BMI_CAT <- as.factor(train_un$BMI_CAT)
```

##### Of all the models we'll be using, few are distance based algorithms. Hence, it is very necessary to scale the numerical variables in our data. We have tried here 2 types of scaling: __normalization and standardization__. After running the models for both the scaling, standard scale was finally chosen for the data.
```{r}
train_sc <- train_un

for(i in 1:length(colnames(train_sc))){
  if(class(train_sc[,i]) == "numeric" || class(train_sc[,i]) == "integer"){
    train_sc[,i] <- as.vector(scale(train_sc[,i], center = T))
  }
}

##the normalization function is created
#norm_ZN <- function(x){
#  y <- c()
#  for (i in 1:length(x)){
#    y <- c(y, ((x[i]) - min(x))/(max(x) - min(x)))
#  }
#  return(y)
#}
#for(i in 1:length(train_sc)){
#  if(class(train_sc[,i]) == "numeric" || class(train_sc[,i]) == "integer"){
#    train_sc[,i] <- norm_ZN(train_sc[,i])
#  }
#}
```

##### Now, we'll be creating the __dummy variables__ for all the categorical variables our data. After we are done with that we need to drop the original categorical variables. 
```{r}
dummies <- c('Gender','Cholesterol','Glucose', 'BP_Category', 'BMI_CAT')
train_dm <- dummy_cols(train_sc, select_columns = dummies) 
train_dm <- train_dm[,-which(names(train_dm) %in% dummies)]
#str(train_dm)
#View(train_dm)
```

##### The newly created dummy variables are all of 'factor' data type. We need to  convert them to 'integer' data type. But we will keep our traget variable as a factor data type only.
```{r}
i <- sapply(train_dm, is.integer)
train_dm[i] <- lapply(train_dm[i], as.factor)
```

##### Since this is a binary classification problem, we can use __logistic regression__ model to check the importance of the variables. This will give us confidence in our newly created variables and eliminate the variables that are not significant.
```{r}
summary(glm(Disease ~. , data=train_dm, family=binomial))
```

##### It seems as the height variable is not significant but weight and BMI are siginificant. BMI catches a combined effect of __height and weight__ so we can eliminate height and weight variable. Also, the newly created variable __'BMI_CAT'__ dosen't seem to work well for the classification, hence eliminating that variable as well. For __'BP_Category'__ variable, combining the normal and elevated category into one as that will increase the significance of other 3 categories.
```{r}
lr_drop <- c('Height', 'Weight', 'BMI_CAT_Healthy', 'BMI_CAT_Obese', 'BMI_CAT_Overweight', 'BMI_CAT_Underweight')
train_dm <- train_dm[,-which(names(train_dm) %in% lr_drop)]

for (i in 1:nrow(train_dm)) {
  if (train_dm$BP_Category_Elevated[i] == "1") {
    train_dm$BP_Category_Normal[i] = "1"
  }
  else {
    train_dm$BP_Category_Normal[i] = train_dm$BP_Category_Normal[i]
  }
}
train_dm$BP_Category_Elevated <- NULL
train_dm_saved <- train_dm
#str(train_dm)
```

##### Splitting the traing data into two parts, training and validation datasets. Here we are using __75-25__ split.
```{r}
set.seed(73)
train_index <- createDataPartition(train_dm$Disease, p = 0.75, list = FALSE)

train_dm_train <- train_dm[train_index, ]
train_dm_validate <- train_dm[-train_index, ]
```

### __3. Model building and Evaluation__

### __Logistic Regression__

### __Base Model:__
##### First Applying the base model (without any hyperparameters) to the training dataset. We observe that our Accuracy is close to __72.6%__ on the base model.
```{r include=FALSE}
glm_base = train(Disease ~ ., data = train_dm_train, method = "glm")
#glm_base
```

#### __Summary Interpretation:__
##### It looks like most of our variables are significant except __Gender__ and __Low Blood Pressure__. We know from correlation plot that Low blood pressure has a very __high correlation__ with High blood pressure. Hence, in regression automatically one of the two variables becomes less significant. The significant variables have the __p-value less than 0.05__. We call them significant because we can say that we are __95%__ confident that the coefficient of those variables lie within the __confidence interval__. Unfortunately, the summary of logistic regression does not show us the confidence interval's lower and upper limit. Also, we can see that coefficient of all the variables are low, that is because we are performing regression on the __scaled data__.
```{r}
summary(glm_base)
```

##### __Performance Evaluation Parameters:__ For this problem statement, when we have to predict if the patient is suffering from the disease or not, it very important to reduce the type 2 error. For that we have the optimize the __recall__ value. Also, since the dataset is balanced, __accuracy__ is the best evaluation parameter.
##### __Predictions:__ Now predicting the values on validation dataset using the base model.
```{r}
glm_predict <- predict(glm_base, newdata = train_dm_validate)
confusionMatrix(glm_predict, train_dm_validate$Disease, positive = "1")
```


### __Logistic Regression Regularised model:__
##### __Regularization Strategy:__ We need to regularise the logistic regression. Unfortunately, with only caret package we cannot define the regularisation parameters. So, we have to use __glmnet package__ and define method glmnet in train function of caret package. We will be able to tune two hyperparameters for regularization:
#####__Hyperparameters:__
##### 1] __lambda__: This paramater controls the __instensity__ of regularization. It means that a penalty term is added to __cost function__ in logistic regression. This added penalty tern helps in reducing the coefficients of less important variables, so that they will not affect the predictions by much.
##### 2] __alpha:__ alpha acts as a __slider between L1 and L2__ regularization. L1 regularization can make the coefficients of variables __completely 0__, hence making them dead variables. L2 regularization also reduces the coefficients but cannot make it completely 0. 
```{r}
glmGrid <-  expand.grid(alpha = c(0.05,0.1,0.2), lambda = c(0.001,0.01, 0.1))

glmnet_reg = train(Disease ~ ., data = train_dm_train, method = "glmnet", trControl =
                     trainControl(method = "cv", number = 3), tuneGrid = glmGrid, metric =
                     "Accuracy")
#glmnet_reg
plot(glmnet_reg)
```

#### __Strategy for Regularization:__
##### So, we have to decide the values for lambda and alpha, which in turn means that we have to set for strength and type of regularization. If  __alpha is 0__, it means we are using __L2 regularization__. If __alpha is 1__, it means we are using __L1 regularization__. Now, since we have limited variables and most of them are significant, we have to focus on using __L2 regularization and a weak regularization__ because the coefficients of the limited variables are important. The train model summary shows __trend in Accuracy__ with variation in alpha and lambda. Training accuracy increases with increase in alpha upto a certain limit and decreases with increase in lambda. It means that model wants us to use L2 regularization with low intensiy.


##### Printing the hyperparameter values for best model
```{r}
get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}
get_best_result(glmnet_reg)
```

##### __Predictions:__ Now, predicting the values on validation data using the regularized model. One thing that we observe is that our accuracy does not increase by much. Hence, we can conclude that the base model was __not overfitted__ on training data.
```{r}
glmnet_reg_predict <- predict(glmnet_reg, newdata = train_dm_validate)
confusionMatrix(glmnet_reg_predict, train_dm_validate$Disease, positive = "1")
```

##### __ROC and AUC:__
##### Plotting the roc curve and calculating the area under that curve. First, creating for it and using the same function for the next models.
```{r}
ROC_AUC <- function(true_val, probs){
  roc_curve=roc(response=true_val, predictor= probs)
  fig <- plot(roc_curve, col="red", lwd=3, main="ROC curve")
  score <- auc(roc_curve)
  ro_au <- list(fig, score)
  return(ro_au)
}
glmnet_probs <- predict(glmnet_reg, newdata = train_dm_validate, type = 'prob')
ROC_AUC(train_dm_validate$Disease, glmnet_probs$`1`)
```



### Deep Learning: Artificial Neural Networks

##### __Data Pre-Processing:__
##### Transforming the data for neural networks. Converting all the factor dummy variables to __integer data type__. Trying the __'receipe'__ library preprocessing the data. Since, our data is already pre-processed and cleaned we do not need to do much.
##### Splitting the data training and validation to check for the overfitting of the deep learning models.
```{r}
train_dm_saved <- train_dm
i <- sapply(train_dm_saved, is.factor)
train_dm_saved[i] <- lapply(train_dm_saved[i], as.character)

i <- sapply(train_dm_saved, is.character)
train_dm_saved[i] <- lapply(train_dm_saved[i], as.integer)


train_dm_tbl <- train_dm_saved %>% drop_na() 

set.seed(100)
train_test_split <- initial_split(train_dm_tbl, prop = 0.75)
train_tbl <- training(train_test_split)
validate_tbl <- testing(train_test_split)

rec_obj <- recipe(Disease ~ ., data = train_tbl) %>% prep(data = train_tbl)

train_x <- bake(rec_obj, new_data = train_tbl) #%>% select(-Disease)
validate_x <- bake(rec_obj, new_data = validate_tbl) #%>% select(-Disease)
train_y <- ifelse(pull(train_tbl, Disease) == "1", 1, 0)
validate_y <- ifelse(pull(validate_tbl, Disease) == "1", 1, 0)

train_x <- train_x[, -which(colnames(train_x) == "Disease")]
validate_x <- validate_x[, -which(colnames(validate_x) == "Disease")]
```

### __ANN: 0 Hidden Layers__
##### __Model Description:__
##### Aritificial neural network with 0 hidden layers is same as a __perceptron model__. There will be a input layer and a output layer. Input layer is nothing but the training example, so we do not need to specify it. But, in this case the activation function is __sigmoid function__ instead of the sign function. The __linear combination__ of input values and weights of the links is processed through and converted to required format by activation function.
##### __Strategy:__ Since, grid search is not possible in keras, we will be using nested for loops for tuning the hyperparamters.
##### __Hyperparameters:__
##### 1] epochs: Number of epochs is the numer of times that whole dataset has completed a forward and backward journey through the network.  
##### 2] batch size: It is not possible to send whole data at once through the network, hence we create small batches of training examples. Number of example in any batch is known as batch size.
##### __Nodes and Layers:__
##### In the input layer we will have nodes = number of variables and in the output node there will be only one node since, it is a binary classification problem. For a multiple class classification, nodes in output function = number of classes.
##### __Activation Function:__
##### For output layer, it will be __Sigmoid Function__, because this is a binary classification problem and output of sigmoid function is probability between 0 and 1. We will use the threshold of 0.5 to classify as either 1 or 0.
##### __Regularization:__
##### It will not be performed for 0 hidden layers, but will be peformed for model with hidden layers.
##### __Optimzation:__
##### The process of changing the weights of connecting links is defined by optimizer. For example, __learning rate__ parameter of gradient descent tells us by how much should  we change the weights. Higher values take less time to run, but might ___overshoot the global/local minima__ and hence it is very important to define right value for learning rate and finding the right optimizer. In this case, we are using __adam__ as our optimization function.
##### __Loss Function:__
##### We have to define a loss function that will calculate the difference between __actual value and predicted value__. That function will use this difference to __update the weights__ of the connecting links. In this case, since it is a binary classification problem, we use __binary_crossentropy__ loss function.
```{r}
batch_size_grid <- c(32,64,128)
epochs_grid <- c(80,100,120)

ann0_models <- list()
ann0_fits <- list()

p <- 1
for (i in 1:length(batch_size_grid)){
  for (j in 1:length(epochs_grid)){
   keras0_model <- keras_model_sequential()
    keras0_model %>%
      layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>%
      compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("accuracy"))
  
    keras0_fit <- fit(object = keras0_model, x = as.matrix(train_x), y = train_y, batch_size =
                        batch_size_grid[i], epochs = epochs_grid[j], validation_split = 0.25,
                        verbose = F)
    ann0_models[[p]] <- keras0_model
    ann0_fits[[p]] <- keras0_fit
    p <- p + 1
  }  
}
```

##### __Finding the Best Model:__ 
##### Now, after fine tuning the model with right hyperparameters, we need to extract the best model. For this we will use the __average of validation accuracy__ of every model for all the epochs.
##### Important Observation: After running the multiple times for different hyperparameter values, we found that best batch_size stays constant, while best epochs number keeps on fluctuating between these 3 values.
```{r}
avg_acc <- c()
for (i in 1:length(ann0_models)){
  valacc <- ann0_fits[[i]]$metrics$val_accuracy
  avg_acc[i] <- sum(valacc)/length(valacc)
  best_model_Index <- which.max(avg_acc)
  keras0_best_model <- ann0_models[best_model_Index][[1]]
  keras0_best_fit <- ann0_fits[best_model_Index][[1]]
}
paste0("Batch Size for best model is: ", keras0_best_fit$params$batch_size)
paste0("No. Epochs for best model is: ", keras0_best_fit$params$epochs)
```

##### Plotting the fitted model over the training dataset. This plots gives us the comparison between training accuracy/loss and validation accuracy/loss. If the difference between training and validation accuracy is increasing that means the model is getting overfitted.
```{r}
plot(keras0_best_fit)
```

##### __Predictions:__ 
##### Now, making prediction on the validation data using the best model. Predicting the classes as well as the probabilities for the instances.
```{r}
pred_class_keras0 <- predict_classes(object = keras0_best_model, x = as.matrix(validate_x)) %>%
as.vector()
pred_prob_keras0 <- predict_proba(object = keras0_best_model, x = as.matrix(validate_x)) %>%
as.vector()
```

##### __Performance Evaluation Parameters:__
##### Plotting the confusion matrix and calculating __Accuracy, Recall and F1-Score__. We are calculating F1-Score because it is the harmonic mean of recall and precision, hence a balanced measure of both the quantities.
```{r}
estimates_keras0_tbl <- tibble(truth = as.factor(validate_y), estimate =
                                 as.factor(pred_class_keras0), class_prob = pred_prob_keras0)
options(yardstick.event_first = F)
estimates_keras0_tbl %>% conf_mat(truth, estimate)
estimates_keras0_tbl %>% metrics(truth, estimate)
estimates_keras0_tbl %>% recall(truth, estimate)
estimates_keras0_tbl %>% f_meas(truth, estimate, beta = 1)
```

##### __ROC and AUC:__
##### Plotting the ROC curve and calculating the area under curve.
```{r}
ROC_AUC(validate_y, pred_prob_keras0)
```

### __Comparison between Linear SVM, Logistic Regression and Single Layer Perceptron:__
##### We will compare the accuracies of all three model on valisation dataset.
```{r}
#transforming svm prediction to match new changes
svm_predict_trans <- as.factor(as.integer(svm_lin_tuned_predict) - 1)

svmLin <- confusionMatrix(svm_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
logReg  <- confusionMatrix(glmnet_reg_predict, train_dm_validate$Disease)$overall["Accuracy"]
annZero <- (estimates_keras0_tbl %>% metrics(truth, estimate))$.estimate[1]

paste0("The validation accuracy for SVM_Linear: ", svmLin)
paste0("The validation accuracy for Logistic_R: ", logReg)
paste0("The validation accuracy for ANN0_Layer: ", annZero)
```
##### __Theoretical Similarity:__
##### __Linear SVM:__ This algorithm is maximum margin classifier. It uses a linear hyperplane to seperate two classes i.e. it creates a linear decision boundary. Linear SVM adds a layer of margin on either side of the decision boundary to reduce the generalization error.
##### __Logistic Regression:__ Being a regression algorithm, logistic regression with multiple input variables, tries to fit a linear hyperplane such that the sum of squared error is minimum for the fitted hyperplane. Then it uses sigmoid function to map the output of linear combination of all variables and weights between 0 and 1. A threshold is set between 0 and 1 (generally 0.5), to sperate two classes.
##### __Perceptron 0 Hidden Layer:__ Perceptron with 0 hidden layer linearly combines the weights and the corresponding input data values, passes it to the output layer. Output layer consists of an activation function, that generates the output in required form and uses a linear hyperplane to classify.
##### __Inference:__ All the three methods use linear hyperplane for classification. The technique of fitting a hyperplane for logistic and Perceptron0 is very similar. But, linear SVM uses maximum margin technique to find the right inclination for linear boundary. Even then, the performance of all three models on same dataset are comparable.

### __ANN: One Hidden Layer__
##### __Model Description:__
##### Aritificial neural network with 1 hidden layers is type of __Multilayer Perceptron Network__. Along with an input layer and an output layer, there will be one more Layer in __between__ input and output.
##### __Strategy:__ Since, grid search is not possible in keras, we will be using nested for loops for tuning the hyperparamters.
##### __Hyperparameters:__
##### Same as before (epochs and batch size)
##### __Nodes and Layers:__
##### In the input layer we will have nodes = number of variables and in the output node there will be only one node since, it is a binary classification problem. Now, since there is only 1 hidden layer, we keep the __nodes in hidden layer__ same as that of input layer.
##### __Activation Function:__
##### For output layer, it will be the same __Sigmoid Function__, but this time there will be one more activation function for the hidden layer. In this case, we propose to use __'Rectified Linear Unit'(Relu)__ acivation function. We use this function because we want a __non-linear__ function but should also maintain the __wide range__ of linear combination of weights and values. Sigmoid and Hyperbolic tangent functions have limited range.
##### __Regularization:__
##### layer_dropout: Temporarily some random nodes are dropped out from a particular layer i.e. they are not considered in calculation for forward pass nor are they considered for updating weights in backward pass. This helps other nodes in that layer to develp relation within network. Here, we specify the probability of dropout to be 10%.
##### __Optimzation:__
##### The optimizer remains the same: __adam__
##### __Loss Function:__
##### Loss function also remains the same: __binary_crossentropy__
```{r}
batch_size_grid <- c(32,64,128)
epochs_grid <- c(60,80,100)

ann1_models <- list()
ann1_fits <- list()

p <- 1
for (i in 1:length(batch_size_grid)){
  for (j in 1:length(epochs_grid)){
    keras1_model <- keras_model_sequential()
    keras1_model %>%
    layer_dense(units = 19, kernel_initializer = "uniform", activation = "relu", input_shape =
                  ncol(train_x)) %>%
    layer_dropout(rate = 0.1) %>%
    layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>%
    compile(optimizer = "sgd", loss = "binary_crossentropy", metrics = c("accuracy"))

    keras1_fit <- fit(object = keras1_model, x = as.matrix(train_x), y = train_y, batch_size =
                        batch_size_grid[i], epochs = epochs_grid[j], validation_split = 0.25,
                      verbose = F)
    
    ann1_models[[p]] <- keras1_model
    ann1_fits[[p]] <- keras1_fit
    p <- p + 1
  }
}
```

##### __Finding the Best Model:__ 
##### Again using the __average validation accuracy__ over all epochs as metric to select best model.
##### Important Observation: After running the multiple times for different hyperparameter values, we found that best batch_size is 64, while best epochs number keeps on fluctuating between these 3 values.
```{r}
avg_acc <- c()
for (i in 1:length(ann1_models)){
  valacc <- ann1_fits[[i]]$metrics$val_accuracy
  avg_acc[i] <- sum(valacc)/length(valacc)
  best_model_Index <- which.max(avg_acc)
 keras1_best_model <- ann1_models[best_model_Index][[1]]
  keras1_best_fit <- ann1_fits[best_model_Index][[1]]
}
paste0("Batch Size for best model is: ", keras1_best_fit$params$batch_size)
paste0("No. Epochs for best model is: ", keras1_best_fit$params$epochs)
```

##### Plotting the fitted model over the training dataset. This plots gives us the comparison between training accuracy/loss and validation accuracy/loss. If the difference between training and validation accuracy is increasing that means the model is getting overfitted.
```{r}
plot(keras1_best_fit)
```

##### __Predictions:__ 
##### Now, making prediction on the validation data using the best model. Predicting the classes as well as the probabilities for the instances.
```{r}
pred_class_keras1 <- predict_classes(object = keras1_best_model, x = as.matrix(validate_x)) %>%
as.vector()
pred_prob_keras1 <- predict_proba(object = keras1_best_model, x = as.matrix(validate_x)) %>%
as.vector()
```

##### __Performance Evaluation Parameters:__
##### Plotting the confusion matrix and calculating __Accuracy, Recall and F1-Score__. Again, F1-Score gives a balanced measure of recall and precision.
```{r}
estimates_keras1_tbl <- tibble(truth = as.factor(validate_y), estimate =
                                 as.factor(pred_class_keras1), class_prob = pred_prob_keras1)
options(yardstick.event_first = F)
estimates_keras1_tbl %>% conf_mat(truth, estimate)
estimates_keras1_tbl %>% metrics(truth, estimate)
estimates_keras1_tbl %>% recall(truth, estimate)
estimates_keras1_tbl %>% f_meas(truth, estimate, beta = 1)
```

##### __ROC and AUC:__
##### Plotting the ROC curve and calculating the area under curve.
```{r}
ROC_AUC(validate_y, pred_prob_keras1)
```

### __ANN: Two Hidden Layer__
##### __Model Description:__
##### This multilayer perceptron model will be having two hidden layers between input layer and output layer.
##### __Strategy:__ Since, grid search is not possible in keras, we will be using nested for loops for tuning the hyperparamters.
##### __Hyperparameters:__
##### 1] batch size, 2] epochs and;
##### 3] Nodes in Hidden Layer 2: In addition to batch size and epochs, we will tunenumber of nodes in second hidden layer.
##### __Nodes and Layers:__
##### Input Layer: Same as number of variables, Output Layer: 1 Node, Hidden Layer 1: equal to number of nodes in Input Layer, Hidden Layer 2: Finding the best value using hyperparameter tuning.
##### __Activation Function:__
##### For output layer, it will be the same __Sigmoid Function__, and both the hidden layers we will be using __'Relu'__ acivation function.
##### __Optimzation:__
##### The optimizer remains the same: __adam__
##### __Loss Function:__
##### Loss function also remains the same: __binary_crossentropy__
```{r}
batch_size_grid <- c(64,128)
epochs_grid <- c(40,60)
hid_nod_grid <- c(10,15,19) 

ann2_models <- list()
ann2_fits <- list()

p <- 1
for (i in 1:length(batch_size_grid)){
  for (j in 1:length(epochs_grid)){
   for (k in 1:length(hid_nod_grid)){
      keras2_model <- keras_model_sequential()
      keras2_model %>%
        layer_dense(units = 19, kernel_initializer = "uniform", activation = "relu", input_shape
                    = ncol(train_x)) %>%
        layer_dropout(rate = 0.2) %>%
        layer_dense(units = hid_nod_grid[k], kernel_initializer = "uniform", activation =
                      "relu") %>%
        layer_dropout(rate = 0.1) %>%
        layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>%
        compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("accuracy"))
  
      keras2_fit <- fit(object = keras2_model, x = as.matrix(train_x), y = train_y, batch_size =
                          batch_size_grid[i], epochs = epochs_grid[j], validation_split = 0.25,
                          verbose = F)
      ann2_models[[p]] <- keras2_model
      ann2_fits[[p]] <- keras2_fit
      p <- p + 1
    }
  }  
}
```

##### __Finding the Best Model:__ 
##### Again using the __average validation accuracy__ over all epochs as metric to select best model.
```{r}
avg_acc <- c()
for (i in 1:length(ann2_models)){
  valacc <- ann2_fits[[i]]$metrics$val_accuracy
  avg_acc[i] <- sum(valacc)/length(valacc)
}
best_model_Index <- which.max(avg_acc)
keras2_best_model <- ann2_models[best_model_Index][[1]]
keras2_best_fit <- ann2_fits[best_model_Index][[1]]

paste0("Batch Size for best model is: ", keras2_best_fit$params$batch_size)
paste0("No. Epochs for best model is: ", keras2_best_fit$params$epochs)
```

##### Plotting the fitted model over the training dataset. This plots gives us the comparison between training accuracy/loss and validation accuracy/loss. If the difference between training and validation accuracy is increasing that means the model is getting overfitted.
```{r}
plot(keras2_best_fit)
```

##### __Predictions:__ 
##### Now, making prediction on the validation data using the best model. Predicting the classes as well as the probabilities for the instances.
```{r}
pred_class_keras2 <- predict_classes(object = keras2_best_model, x = as.matrix(validate_x)) %>%
as.vector()
pred_prob_keras2 <- predict_proba(object = keras2_best_model, x = as.matrix(validate_x)) %>%
as.vector()
```

##### __Performance Evaluation Parameters:__
##### Plotting the confusion matrix and calculating __Accuracy, Recall and F1-Score__. Again, F1-Score gives a balanced measure of recall and precision.
```{r}
estimates_keras2_tbl <- tibble(truth = as.factor(validate_y),estimate =
                                 as.factor(pred_class_keras2),class_prob = pred_prob_keras2)

options(yardstick.event_first = F)
estimates_keras2_tbl %>% conf_mat(truth, estimate)
estimates_keras2_tbl %>% metrics(truth, estimate)
estimates_keras2_tbl %>% recall(truth, estimate)
estimates_keras2_tbl %>% f_meas(truth, estimate, beta = 1)
```

##### __ROC and AUC:__
##### Plotting the ROC curve and calculating the area under curve.
```{r}
ROC_AUC(validate_y, pred_prob_keras2)
```

### __ANN: Three Hidden Layer (Voluntary work for Testing)__
##### __Model Description:__
##### This multilayer perceptron model will be having three hidden layers between input layer and output layer.
##### __Strategy:__ We will not be performing hyperparameter tuning, instead using the best values from previous models.
##### __Hyperparameters:__
##### N/A
##### __Nodes and Layers:__
##### Input Layer: Same as number of variables, Output Layer: 1 Node, Hidden Layer 1: equal to number of nodes in Input Layer, Hidden Layer 2: , Hidden Layer 3: .
##### __Activation Function:__
##### For output layer, it will be the same __Sigmoid Function__, and all the hidden layers we will be using __'Relu'__ acivation function.
##### __Optimzation:__
##### The optimizer remains the same: __adam__
##### __Loss Function:__
##### Loss function also remains the same: __binary_crossentropy__
```{r}
keras3_model <- keras_model_sequential()
keras3_model %>%
  layer_dense(units = 19, kernel_initializer = "uniform", activation = "relu", input_shape =
                ncol(train_x)) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 10, kernel_initializer = "uniform", activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 15, kernel_initializer = "uniform", activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1, kernel_initializer = "uniform", activation = "sigmoid") %>%
  compile(optimizer = "adam", loss = "binary_crossentropy", metrics = c("accuracy"))

keras3_fit <- fit(object = keras3_model, x = as.matrix(train_x), y = train_y, batch_size = 64,
                epochs = 100, validation_split = 0.25, verbose = F)
```

##### __Performance Evaluation:__
##### We will be checking the performance of ANN3 model on the validation data.
##### __Results Interpretation:__
##### So, we observe that for this dataset, even if we add a third hidden layer, it does not make any significant changes to the output results. We can infer that 2 hidden layers are enough to make the required combination of values and weights to generate the output. I believe that only 1 hidden layer is enough to improve the accuracy and recall values for this dataset, because we did not see any significant increase in performance after adding second hidden layer as well. 
```{r}
pred_class_keras3 <- predict_classes(object = keras3_model, x = as.matrix(validate_x)) %>%
as.vector()
estimates_keras3_tbl <- tibble(truth = as.factor(validate_y),estimate =
                                 as.factor(pred_class_keras3))
options(yardstick.event_first = F)
estimates_keras3_tbl %>% metrics(truth, estimate)
estimates_keras3_tbl %>% recall(truth, estimate)
```


### __Decision Tree__
### __Base Model__
##### First testing the base of decision tree on training data and testing the model on validation data.
```{r}
colnames(train_dm_train) <- make.names(colnames(train_dm_train))
colnames(train_dm_validate) <- make.names(colnames(train_dm_validate))
dt_base <- train(Disease~., data = train_dm_train, metric = "Accuracy", method = "rpart")
dt_base
```

##### __Performance Evaluation:__Checking the performance of model on the validation dataset. Evaluation we'll be using remains same Accuracy and Recall. 
```{r}
dt_base_predict <- predict(dt_base, newdata = train_dm_validate, type = "raw")
confusionMatrix(dt_base_predict, train_dm_validate$Disease, positive = "1")
```

### __Decision Tree: Tuned Model__
##### __Model Description:__
##### This model generated a classification tree. The root node contains the variable split that contains highest information gain or lowest GINI index after splitting. All the leaf nodes are assigned to class from target variable. 
##### __Strategy:__ In this case, we will be pre-prunning the tree and not post-prunning. We will use cross-validation to control the model from getting overfitted over the data. There are multiple hyperparameters in "rpart" method to prune the tree. We will be complexity parameter only to prune the tree as it automatically sets the values for few other hyperparameters.
##### __Hyperparameters:__
##### 1] cp (complexity parameter): This parameter decides if the the tree should be expanded further from the current node. If the cost of expanding the tree is more than complexity parameter, then it stops the tree from increasing further. The cost is calculated using the impurity criteria such as Information Gain (entropy) or GINI Index.
##### 2] minsplit: It is the minimum number of observations required to split a node further.
##### 3] minbucket: It is minimum number of observations required to be there in leaf nodes. Genarally, this number is taken as one third of the minsplit value.
##### 4] maxDepth: We will not specify this hyperparameter, and let the tree grow fully and complexity parameter will take care of the length of the tree.

##### Using the best cp value from above results, we specify few values higher and few lower to check for cp value and finally use for loop to find the optimum values for minsplit and minbucket
```{r}

tunegrid <- expand.grid(cp = seq(0.001, 0.005, 0.001))

dt_models <- list()
for (minsplit in c(20,30,40)){
  set.seed(73)
  dt_tuned <- train(Disease~., data=train_dm_train, method="rpart",
              tuneGrid=tunegrid, trControl = trainControl(method="cv", number=3), 
              control= rpart.control(minsplit = minsplit, minbucket =
              round(minsplit/3)))
  min_splits <- toString(minsplit)
  dt_models[[min_splits]] <- dt_tuned
}
```

##### __Best Model:__ Since, we used the for loop, we need to find the best model ourselves and then select the final model out of it.
```{r}
all_acc <- c()
for (i in 1:length(dt_models)){
  acc_vec <- dt_models[[i]]$results$Accuracy
  all_acc <- c(all_acc, acc_vec)
}


best_dt_Index <- ceiling(which.max(all_acc)/length(acc_vec))
dt_tuned_best <- dt_models[best_dt_Index][[1]]
dt_tuned_best
```

##### Plotting the diagram for the best decision tree model.
```{r}
library(rattle)
fancyRpartPlot(dt_tuned_best$finalModel)
```

##### __Preictions:__ 
##### Testing the model performance on the validation dataset.
```{r}
dt_tune_predict <- predict(dt_tuned_best, newdata = train_dm_validate, type = "raw")
confusionMatrix(dt_tune_predict, train_dm_validate$Disease, positive = "1")
```

##### __ROC and AUC:__
##### Plotting the ROC curve and calculating the area under curve.
```{r}
dt_probs_predict <- predict(dt_tuned_best, newdata = train_dm_validate, type = "prob")
ROC_AUC(train_dm_validate$Disease, dt_probs_predict$`1`)
```

### __Comparing the Variable significane/Importance:__
##### Output is commented because is it falls under the category of "Excessive output of less importance"
##### __Random Forest:__ High BP, Age and BMI are the top three important variables here. While we can observe that Gender and Glucose are not that important variables. 
##### __Decision Tree:__ For Decision tree, High blood pressure, Hypertension stage 2 and Low blood pressure are the most important variable, while variables like gender and cholesterol were not used to build the decision tree.
##### __Gradient Boosting:__ For GBM,again High blood pressure, Age and Cholesterol category "too_high" are very imporatnt with respect to fiding out if that person has a disease. 
##### __Logistic Regression:__ Coefficients tells us the importance of variables: High blood pressure, HTStage2 and Hypertensive have highest coefficient __ignoring the sign__ of coefficient. P-value tells us the significane of the coefficient value. We say that Low blood pressure and gender variables are not significant.
```{r}
#varImp(rf_tuned_best$finalModel)     #Random Forest
#varImp(dt_tuned_best$finalModel)     #Decision Tree
#varImp(gbm_tuned$finalModel)         #Gradient Boosting 
#summary(glm_base$finalModel)         #Logistic Regression
```

### __Master Table:__ For Comapring Model Performance
##### __Performance Evaluation Method:__
##### Here, we have used cross validation as performance evaluation method, wherever possible.
##### __Primary Performance Metric:__
##### This problem statement asks us to reduce the type 2 error, hence it is very necesaary to optimize recall. Adding to that this is a balanced dataset and for balanced dataset, Accuracy is always a good metric as it covers all the dimensions of predictons. In the last assignment, we saw a pattern that if we tuning for recall decreases accuracy, but tuning for accuracy does not decrease recall by much. Hence, we tuned our models for accuracy and kept our accuracy as our primary performance metric.
```{r}
knn_predict_trans <- as.factor(as.integer(knn_acc_predict) - 1)
nb_predict_trans <- as.factor(as.integer(nb_tuned_best_predict) - 1)
rf_predict_trans <- as.factor(as.integer(rf_tuned_predict) - 1)
svmnl_predict_trans <- as.factor(as.integer(svm_nl_tuned_predict) - 1)
gbm_predict_trans <- as.factor(as.integer(gbm_tuned_predict) - 1)

knnacc <- confusionMatrix(knn_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
nbacc <- confusionMatrix(nb_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
rfacc <- confusionMatrix(rf_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
svmlacc <- confusionMatrix(svm_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
svmnlacc <- confusionMatrix(svmnl_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
gbmacc <- confusionMatrix(gbm_predict_trans, train_dm_validate$Disease)$overall["Accuracy"]
lracc <- confusionMatrix(glmnet_reg_predict, train_dm_validate$Disease)$overall["Accuracy"]
ann0acc <- (estimates_keras0_tbl %>% metrics(truth, estimate))$.estimate[1]
ann1acc <- (estimates_keras1_tbl %>% metrics(truth, estimate))$.estimate[1]
ann2acc <- (estimates_keras2_tbl %>% metrics(truth, estimate))$.estimate[1]
dtacc <- confusionMatrix(dt_tune_predict, train_dm_validate$Disease)$overall["Accuracy"]


Algorithm_Name <- c("K-Nearest Neighbor", "Naive Bayes Classifier", " Random Forest", " SVM Linear", "SVM Non-Linear", "Gradient Boosting", "Logistic Regression", "ANN 0 Hidden Layer", "ANN 1 Hidden Layer", "ANN 2 Hidden Layer", "Decision Tree")

Hyperparameters <- c("K- Number of Neighbors", "Laplace Smoother", "Number of trees(ntree), number of features for each tree(.mtry)", "C - Cost Parameter", "C - Cost Parameter, Lambda - Gaussian Distribution Parameter", "interaction.depth - splits, ntree - trees in gbm, shrinkage - learning rate, n.minobsinnode - min sample in leaf nodes", "lambda - regularization strength, alpha - type of regularization", "batch_size - interval for weight updation, epochs - number of epochs", "batch_size - interval for weight updation, epochs - number of epochs", "batch_size - interval for weight updation, epochs - number of epochs, unit - nodes in 2nd hidden layer", "cp - complexity parameter, minsplit - min sample for splitting, minbucket - min samples in leaf node")

Model_Performance_Accuracy <- c(knnacc, nbacc, rfacc, svmlacc, svmnlacc, gbmacc, lracc, ann0acc, ann1acc, ann2acc, dtacc)

Ballpark_Time_Estimate_minutes <- c(40, 0.1, 120, 180, 240, 30, 0.5, 30, 30, 30, 0.5)

master_table <- data.frame(Algorithm_Name, Hyperparameters, Model_Performance_Accuracy, Ballpark_Time_Estimate_minutes)

master_table <- master_table[order(Model_Performance_Accuracy, decreasing = T),]
master_table
#View(master_table)
```

##### Transforming test data with respect to the transformed train data to pass it through the various models make our final predictions
```{r}
test_df <- read.csv('Disease Prediction Testing.csv', stringsAsFactors = F)

#apply(is.na(test_df),2,sum)

#num_test_df <- test_df[sapply(test_df, is.numeric) | sapply(test_df, is.integer)]
#quantiles_test <- c(0.001, 0.002, 0.003, 0.985, 0.99, 0.999, 1)
#out_test_df <- data.frame(row.names = quantiles_test)
#for (i in 1:length(num_test_df)){
#  r <- as.vector(quantile(num_test_df[,i], quantiles_test))
#  out_test_df[,i] <- r
#}
#colnames(out_test_df) <- colnames(num_test_df)
#print(out_test_df)

test_df$Low.Blood.Pressure <- Out_Treat_W(test_df$Low.Blood.Pressure, 0.002, 0.985)
test_df$High.Blood.Pressure <- Out_Treat_W(test_df$High.Blood.Pressure, 0.003, 0.999)

for (i in 1:nrow(test_df)) {
  if (test_df$High.Blood.Pressure[i] >= 180 | test_df$Low.Blood.Pressure[i] >= 120) {
    test_df$BP_Category[i] = "Hypertensive"
  }
  else if (test_df$High.Blood.Pressure[i] >= 140 | test_df$Low.Blood.Pressure[i] >= 90) {
    test_df$BP_Category[i] = "HT_stage2"
  }
  else if (test_df$High.Blood.Pressure[i] >= 130 | test_df$Low.Blood.Pressure[i] >= 80) {
    test_df$BP_Category[i] = "HT_stage1"
  }
  else {
    test_df$BP_Category[i] = "Normal"
  }
}
for (i in 1:nrow(test_df)) {
  test_df$BMI[i] <- (test_df$Weight[i] / ((test_df$Height[i]/100)**2))
}
lr_drop_test <- c('Height', 'Weight')
test_df <- test_df[,-which(names(test_df) %in% lr_drop_test)]

scale_test <- c('Age', 'High.Blood.Pressure', 'Low.Blood.Pressure', 'BMI')
for(i in 1:length(colnames(test_df))){
  if(colnames(test_df)[i] %in% scale_test){
    test_df[,i] <- as.vector(scale(test_df[,i]))
  }
}

dummies_test <- c('Gender','Cholesterol','Glucose', 'BP_Category')
test_df <- dummy_cols(test_df, select_columns = dummies_test)
test_df <- test_df[,-which(names(test_df) %in% dummies_test)]

test_df$ID <- NULL

#Saving the data for ANN
test_df_ann <- test_df
i <- sapply(test_df, is.integer)
test_df[i] <- lapply(test_df[i], as.factor)
test_x <- bake(rec_obj, new_data = test_df_ann)

#prep for decision tree
test_df_dt <- test_df
colnames(test_df_dt) <- make.names(colnames(test_df_dt))
```

##### Now predicting if a person will get the disease or will he/she be safe using actual test data set. Combining all the predictions into a dataframe and exporting the dataframe to a csv file
```{r}
DT <- predict(dt_tuned_best, newdata = test_df_dt)
LR <- predict(glmnet_reg, newdata = test_df)
ANN0 <- predict_classes(object = keras0_best_model, x = as.matrix(test_x)) %>% as.vector()
ANN1 <- predict_classes(object = keras1_best_model, x = as.matrix(test_x)) %>% as.vector()
ANN2 <- predict_classes(object = keras2_best_model, x = as.matrix(test_x)) %>% as.vector()

HW4_CSV <- data.frame(DT, LR, ANN0, ANN1, ANN2)

#View(HW4_CSV)

#write.csv(HW4_CSV,"Assignment4.csv", row.names = TRUE)
```


##### Following code is for all the models from previous assignment, it is not required to run them in this file. We are using the predictions from previous assignment. Hence, this cell is just for the reference. Since, we can save the global enviornment variables and load it, we do not need to  runs all this models again.
```{r}
#set.seed(73)
#knn_acc <- train(Disease ~ ., data = train_dm_train, method = "knn",
#                    tuneGrid = data.frame(k = seq(30, 40)),
#                    trControl = trainControl(method = "repeatedcv",
#                                             number = 3, repeats = 3))
#knn_acc_predict <- predict(knn_acc, newdata = train_dm_validate)


#nb_models <- list()
#for  (i in 1:5){
#  nb_tuned <- naiveBayes(Disease ~ ., data = train_dm_train, laplace = i)
#  lap_par <- toString(i)
#	nb_models[[lap_par]] <- nb_tuned
#}
#Accuracies <- c()
#for (i in 1:length(nb_models)){
#  nb_find_best <- predict(nb_models[[i]], train_dm_validate)
#  cm_nb <- confusionMatrix(nb_find_best, train_dm_validate$Disease)
#  Accuracies <- c(Accuracies, cm_nb$overall["Accuracy"])
#}
#nb_tuned_best <- nb_models[[which.max(Accuracies)]]
#nb_tuned_best_predict <- predict(nb_tuned_best, train_dm_validate)


#mt <- round(sqrt(ncol(train_dm_train)))
#control <- trainControl(method="repeatedcv", number=3, repeats=3)
#tunegrid <- expand.grid(.mtry=c((mt-2):(mt)))
#rf_models <- list()
#for (ntree in c(350, 400, 500)) {
#	set.seed(73)
#	rf_tuned <- train(Disease~., data=train_dm_train, method="rf", #metric="Accuracy",tuneGrid=tunegrid,
#	             trControl=control, ntree=ntree, verbose = TRUE)
#	num_trees <- toString(ntree)
#	rf_models[[num_trees]] <- rf_tuned
#}
#Accuracy_results <- summary(results)$statistics$Accuracy
#best_model_Index <- ceiling(which.max(Accuracy_results)/ncol(Accuracy_results))
#rf_tuned_best <- rf_models[best_model_Index][[1]]
#rf_tuned_predict <- predict(rf_tuned_best, newdata = train_dm_validate)


#set.seed(73)
#svm_lin_tuned <- train(make.names(Disease) ~ ., data = train_dm_train, method = "svmLinear",
#                 trControl = trainControl(method="repeatedcv", number=3, repeats=3), tuneGrid =                  expand.grid(C = seq(0.5, 0.7, 0.05)), metric="Accuracy")
#svm_lin_tuned_predict <- predict(svm_lin_tuned, newdata = train_dm_validate)


#svm_nl_tuned <- train(Disease ~ ., data = train_dm_train,
#                       tuneGrid = expand.grid(sigma = seq(0.1, 0.3, 0.1), C = seq(0.5, 1.5, #0.5)),
#                       method = "svmRadial", trControl = trainControl(method = "repeatedcv",
#                       number = 3, repeats = 3))
#svm_nl_tuned_predict <- predict(svm_nl_tuned, newdata = train_dm_validate)


#tuneGrid_gbm <- expand.grid(interaction.depth=c(4,6,8), n.trees = c(400,500,600),
#                   shrinkage=c(0.01), n.minobsinnode=c(10))

#trainControl_gbm <- trainControl(method="cv", number=3)
#set.seed(28)
#gbm_tuned <- train(Disease ~ ., data = train_dm_train,
#                       tuneGrid = expand.grid(tuneGrid_gbm), method = "gbm",
#                       trControl = trainControl_gbm, verbose=FALSE, metric="Accuracy")
#gbm_tuned_predict <- predict(gbm_tuned, newdata = train_dm_validate)
```

