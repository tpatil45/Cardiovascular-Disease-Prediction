# __Assignment 1__
#### __Author: Tejas Patil__
#### __Date: 04/05/2020__

#### __Executive Summary__:
##### __Problem Statement:__
##### Disease dataset provides the information of people and their physical and habitual attributes that can be helpful in predicting if that person will suffer from a particular type of disease or not. The aim of this project is to build the data analytical models that will help in predicting a person as disease prone or disease safe.
##### __1. Data Load and Cleanup:__
##### This section deals with cleaning the data. We'll check the data for missing values, outliers etc. and treat those observations with suitable values. The data will also be checked for presence of any duplicate values. It also includes the statistically analysing the data.
##### __2. EDA and Feature selection:__
##### This section involves a deeper understanding of the data using tabular and visual representation of the data and __creating new columns__ (tranforming data into a useful form). We'll use __logistic regression__ for the feature selection. Note that we are using logistic regression here because this a binary classification problem and logistic regression can provide good variable importance.
##### __3.Model building and evaluation:__
##### In this section, we will buil 6 different classification models namely: K-Nearest neighbors, Naive Bayes classifier, Random Forest, Supply vector machine linear and non linear, and finally Gradient boosting machine model. Models will be evaluated based on suitable evaluation parameters __(Accuracy and Recall)__. At last, we will use these models for predictions on test dataset.

### __1. Data load and Cleanup__

##### Importing the necessary libraries 
```{r}
library(ggplot2)
library(gridExtra)
library(caret)
library(mlbench)
library(MLmetrics)
library(fastDummies)
library(data.table)
library(corrplot)
library(pROC)
library(klaR)
library(kernlab)
library(gbm)
#install.packages('e1071', dependencies=TRUE)
library(e1071)

setwd('C:/Users/hp/Desktop/IST 707/Assignments/Assignment 3')
#getwd()
```

##### Importing the csv file into R enviornment and checking the structure and statistical summary of the dataset
##### Some early observations from the data:
##### AGE: minimum age is 29 years and maximum is 64 years. Hence our population for this problem is mainly adult people and people in early stages of old age
##### GENDER: training data is pretty imbalanced with respect to the gender variable. Females are almost twice that of male population.
##### Smoke, Alcohol and Excercise are classified as 1 and 0. The variable importance of these variables is doubtful as some people might be doing one of these things in small/large quantity relative to the other quantity.
```{r}
train_df <- read.csv('Disease Prediction Training.csv', stringsAsFactors = F)
#summary(train_df)
#str(train_df)
```

##### Checked the data for Missing values. The results returned shows us that there are __no missing values__ in the data
```{r}
apply(is.na(train_df),2,sum)
```

##### Checked the data for duplicate observations. The results returned shows us that there were __1752 duplicate observations__ in the data. So, we have selected only the unique values
```{r}
nrow(train_df) - nrow(unique(train_df))
train_un <- unique(train_df)
```

### __2. Exploratory Data Analysis__

#### __Analysing the target variable__
##### We find that target variable is of binary type and has 2 levels in it. So, this will be a binary classification problem. Here most important observation is that our target variable is balanced
```{r}
str(train_un$Disease)
table(train_un$Disease)
```

##### For the further EDA, we create 2 new dataframes containing only categorical and only numerical attributes respectively. Before that we convert all binary integer variables to factor for the easying the analysis process
```{r}
train_un$Smoke <- as.factor(train_un$Smoke)
train_un$Alcohol <- as.factor(train_un$Alcohol)
train_un$Exercise <- as.factor(train_un$Exercise)
train_un$Disease <- as.factor(train_un$Disease)


cat_df <- train_un[sapply(train_un, is.character) | sapply(train_un, is.factor)]
num_df <- train_un[sapply(train_un, is.numeric) | sapply(train_un, is.integer)]
```

##### Creating a function to generate bar plot for all the categorical variables and then using the grid plot to display all the visualization.
##### Impotant observations here are that most of the people do not smoke or drink and do excercise regularly. Ideally, the disease variable should have more '0's than '1's which questions the variable impotance for these three variables. 
```{r}
bar_plot <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(x=factor(x))) + stat_count() + xlab(colnames(data_in)[i]) + theme_light() + 
    theme(axis.text.x = element_text(angle = 90, hjust =1))
  return (p)
}
mygrid1 <- list()
for (i in 1:length(cat_df)){
  myplot1 <- bar_plot(cat_df, i)
  mygrid1 <- c(mygrid1, list(myplot1)) 
}
do.call("grid.arrange", c(mygrid1, ncol=2))
```

##### Again creating a function, this time to generate a grid plot for displaying boxplots for all the numerical variables. We see that there are considerable __outliers__ in variables named 'Height', 'Weight', 'High.Blood.Pressure' and 'Low.Blood.Pressure'.
```{r}
box_plot <- function(data_in, i) {
  p <- ggplot(data_in, aes(y = data_in[,i])) + geom_boxplot(color="black", fill="green", alpha=0.2) + ggtitle(colnames(data_in)[i]) + ylab("Distribution") + theme(legend.position="none")
  return (p)
}

mygrid2 <- list()
for (i in 1:length(num_df)){
  myplot2 <- box_plot(num_df, i)
  mygrid2 <- c(mygrid2, list(myplot2)) 
}
do.call("grid.arrange", c(mygrid2, ncol=2))
```



##### Now let us check the range of these outliters. We create a new dataframe and check the values for extreme percentile values from both the ends for variables containing outliers
```{r}
quantiles <- c(0.0, 0.001, 0.002, 0.003, 0.01, 0.985, 0.99, 0.997, 0.998, 0.999, 1)
out_df <- data.frame(row.names = quantiles)
for (i in 1:length(num_df)){
  q <- as.vector(quantile(num_df[,i], quantiles))
  out_df[,i] <- q
}
colnames(out_df) <- colnames(num_df)
out_df <- t(out_df) #taking the transpose
print(out_df)
```
##### Important observations from this dataframe:
##### Age: There are no outliers for age column, all values seem to be genuine
##### Height: The upper bound for the height variable seems to be ok, the lower bound for height variable is very extremely small. The shortest person in this data has a height of 55 cm. Considering the fact that the youngest person is of 29 years, we winsoroze this end of height variable to 0.001 percentile.
##### Weight: Again for weight variable, the upper bound seems to be fine. The lower bound has values of 10 kgs. A normal person who is atleast 29 years old, weighs more than that. Based on the table, we decide the keep the minimum weight of that person to be 40kgs
##### High.Blood.Pressure: High blood pressure which is also known as Systolic pressure is considered normal when the value is below 120 mm Hg but closer to 120. The minimum value in this column are very extreme and cannot be considered. Hence, we consider the minimum value to be 70 which is 0.003 percentile value. Also, we need to winsorize the upper limit here, because the the upper limit is shown to be 14020 and that is impossible. Any value above 180 is considered to be dangerous. So, we replce it with 220 which is the 0.999th percentile.
##### Low.Blood.Pressure: Low blood pressure is also known as Diastolic presssure. The minimum value for this data is 0, which is practically impossible for a living person. Any value less than but close to 80 is considered normal. Hence to maintain the distinguishibility we winsorize it to the 0.002th percentile. The upper bound, more than 500 values 1000 or 1100 or more than that, which is again impossible. So we winsorize it to the 0.985th percentile. 


##### Creating a function to treat outliers by winsorizing to specific boundaries and using it to treat all numerical variables (except age) to treat outliers
```{r}
Out_Treat_W = function(x, lb, ub){
  LC = quantile(x, lb)
  UC = quantile(x, ub)
  UOut <- which(x > UC)
  LOut <- which(x < LC)
  for (i in 1:length(UOut)){
    x[UOut[i]] <- UC
  }
  for (i in 1:length(LOut)){
    x[LOut[i]] <- LC
  }
  return(x)
}
train_un$Height <- Out_Treat_W(train_un$Height, 0.001, 1)
train_un$Weight <- Out_Treat_W(train_un$Weight, 0.001, 1)
train_un$Low.Blood.Pressure <- Out_Treat_W(train_un$Low.Blood.Pressure, 0.001, 0.985)
train_un$High.Blood.Pressure <- Out_Treat_W(train_un$High.Blood.Pressure, 0.003, 0.999)
```

##### Updating the num_data with winsorized numerical variable. Creating a function to generate Histogram for numerical variables. Implementing that function on every variable in num_df dataset. We observe that all the distribution are  slightly skewed but more or less normally distributed.
```{r}
num_df <- train_un[sapply(train_un, is.numeric) | sapply(train_un, is.integer)]

hist_plot <- function(data_in, i) {
  data <- data.frame(x=data_in[[i]])
  p <- ggplot(data=data, aes(data[,1])) + 
    geom_histogram(bins = 30, col="red", aes(fill=..count..)) +
    xlab(colnames(data_in)[i]) +
    scale_fill_gradient("Count", low="green", high="red")
  return (p)
}
mygrid2 <- list()
for (i in 1:length(num_df)){
  myplot2 <- hist_plot(num_df, i)
  mygrid2 <- c(mygrid2, list(myplot2)) 
}
do.call("grid.arrange", c(mygrid2, ncol=2))
```

##### Plotting the correlation matrix to check for the multicollinearity between numeric independent variables. Only significant correlation is between __Low blood pressure__ and __High blod pressure__ variables.
```{r}
corMatrix <- cor(num_df)
#View(descrCor)
#print(descrCor)
corrplot(corMatrix)
```

#### Creating new variables:

##### We have two variables that measure the blood pressure (High and Low). Now, we will create a single categorical variable that will try to capture the combined effect of both variables on a person's health. We will the classifiy the health and unhealthy blood pressure ranges into 5 different categories namely: __Normal__, __Elevated__ (Slight increase), __Hypertension (Stage 1)__, __Hypertension (Stage 2)__ and __Hypertensive Crisis__ (Critical condition). These ranges are obtained using the domain knowledge from the website:
##### [https://www.heart.org/en/health-topics/high-blood-pressure/understanding-blood-pressure-readings]

##### Checking if the variable can make a impact on our model
##### We observe that the number of people having the disease is very high for people of class Hypertension (Stage 1), Hypertension (Stage 2) and Hypertensive Crisis (Critical condition). This gives us __two conclusions__, 1] the newly created variable can make a big impact on our model and 2] There's very high probability that the disease we are studying, might be related to __cardiac failure__.  
```{r}
for (i in 1:nrow(train_un)) {
  if (train_un$High.Blood.Pressure[i] >= 180 | train_un$Low.Blood.Pressure[i] >= 120) {
    train_un$BP_Category[i] = "Hypertensive"
  }
  else if (train_un$High.Blood.Pressure[i] >= 140 | train_un$Low.Blood.Pressure[i] >= 90) {
    train_un$BP_Category[i] = "HT_stage2"
  }
  else if (train_un$High.Blood.Pressure[i] >= 130 | train_un$Low.Blood.Pressure[i] >= 80) {
    train_un$BP_Category[i] = "HT_stage1"
  }
  else if (train_un$High.Blood.Pressure[i] >= 120 & train_un$Low.Blood.Pressure[i] < 80) {
    train_un$BP_Category[i] = "Elevated"
  }
  else {
    train_un$BP_Category[i] = "Normal"
  }
}
train_un$BP_Category <- as.factor(train_un$BP_Category)
table(train_un$Disease, train_un$BP_Category)
```

##### Now we'll be creating two more new variable called BMI (Body Mass Index). BMI is a measure of body fat considering height and weight of a person, it applies to adult men and women. The second variable is nothing but the categorization of the BMI variable.
```{r}
for (i in 1:nrow(train_un)) {
  train_un$BMI[i] <- (train_un$Weight[i] / ((train_un$Height[i]/100)**2))
  if (train_un$BMI[i] <= 18) {
    train_un$BMI_CAT[i] = "Underweight"
  }
  else if (train_un$BMI[i] <= 24) {
    train_un$BMI_CAT[i] = "Healthy"
  }
  else if (train_un$BMI[i] <= 29) {
    train_un$BMI_CAT[i] = "Overweight"
  }
  else {
   train_un$BMI_CAT[i] = "Obese"
  }
}
train_un$BMI_CAT <- as.factor(train_un$BMI_CAT)
```


##### Of all the models we'll be using, few are distance based algorithms. Hence, it is very necessary to scale the numerical variables in our data. We have tried here 2 types of scaling: __normalization and standardization__. After running the models for both the scaling, standard scale was finally chosen for the data.
```{r}
train_sc <- train_un

for(i in 1:length(colnames(train_sc))){
  if(class(train_sc[,i]) == "numeric" || class(train_sc[,i]) == "integer"){
    train_sc[,i] <- as.vector(scale(train_sc[,i]))
  }
}

##the normalization function is created
#norm_ZN <- function(x){
#  y <- c()
#  for (i in 1:length(x)){
#    y <- c(y, ((x[i]) - min(x))/(max(x) - min(x)))
#  }
#  return(y)
#}
#for(i in 1:length(train_sc)){
#  if(class(train_sc[,i]) == "numeric" || class(train_sc[,i]) == "integer"){
#    train_sc[,i] <- norm_ZN(train_sc[,i])
#  }
#}
```


##### Now, we'll be creating the __dummy variables__ for all the categorical variables our data. After we are done with that we need to drop the original categorical variables. 
```{r}
dummies <- c('Gender','Cholesterol','Glucose', 'BP_Category', 'BMI_CAT')
train_dm <- dummy_cols(train_sc, select_columns = dummies) # it will convert your data to hot encoding
train_dm <- train_dm[,-which(names(train_dm) %in% dummies)]
#str(train_dm)
#View(train_dm)
```

##### The newly created dummy variables are all of 'factor' data type. We need to  convert them to 'integer' data type. But we will keep our traget variable as a factor data type only.
```{r}
i <- sapply(train_dm, is.factor)
train_dm[i] <- lapply(train_dm[i], as.integer)

train_dm$Disease <- as.factor(train_dm$Disease)
train_dm_saved <- train_dm
```

##### Since this is a binary classification problem, we can use __logistic regression__ model to check the importance of the variables. This will give us confidence in our newly created variables and eliminate the variables that are not significant.
```{r}
summary(glm(Disease ~. , data=train_dm, family=binomial))
```

##### It seems as the height variable is not significant but weight and BMI are siginificant. BMI catches a combined effect of __height and weight__ so we can eliminate height and weight variable. Also, the newly created variable __'BMI_CAT'__ dosen't seem to work well for the classification, hence eliminating that variable as well. For __'BP_Category'__ variable, combining the normal and elevated category into one as that will increase the significance of other 3 categories.
```{r}
lr_drop <- c('Height', 'Weight', 'BMI_CAT_Healthy', 'BMI_CAT_Obese', 'BMI_CAT_Overweight', 'BMI_CAT_Underweight')
train_dm <- train_dm[,-which(names(train_dm) %in% lr_drop)]

for (i in 1:nrow(train_dm)) {
  if (train_dm$BP_Category_Elevated[i] == "1") {
    train_dm$BP_Category_Normal[i] = "1"
  }
  else {
    train_dm$BP_Category_Normal[i] = train_dm$BP_Category_Normal[i]
  }
}
train_dm$BP_Category_Normal <- as.integer(train_dm$BP_Category_Normal)
train_dm$BP_Category_Elevated <- NULL
#str(train_dm)
```


##### Splitting the traing data into two parts, training and validation datasets. Here we are using __75-25__ split.
```{r}
set.seed(73)
train_index <- createDataPartition(train_dm$Disease, p = 0.75, list = FALSE)

train_dm_train <- train_dm[train_index, ]
train_dm_validate <- train_dm[-train_index, ]
```

### __3. Model building and Evaluation__

### __a] K - Nearest Neighbors__

##### First Applying the base model (without any hyperparameters) to the training dataset
```{r}
#knn_base <- train(Disease ~ ., data = train_dm_train, method = "knn")
print(knn_base)
```

##### Plotting the graph for accuracy of the model vs the number of neighbors selected in a model. It shows that there a linear increase in accuracy with increase in K value.
```{r}
plot(knn_base)
```

##### Plotting the confusion matrix to evaluate model performance on validation dataset by analysing the evaluation parameters such as accuracy, recall, specificity etc. Looking at the p-value we are more than __99% confident__ that our accuracy is more than a random model (No Information rate).
```{r}
knn_base_predict <- predict(knn_base, newdata = train_dm_validate)
confusionMatrix(knn_base_predict, train_dm_validate$Disease)
```

##### Now, fine tuning the model using hyperparameters and using cross validation and running the model for __'accuracy'__. It is a __lazy algorithm__, that runs only after you provide a test dataset to it. For every observation it'll find the K neighbors and then by __majority voting__ it will decide for the final call. Sometimes __weighted average__ is also used to find the class. Weights are given based on the distance of neighbors respectively. After various runs of algorithm over the data, we observed that optimum value of K was fluctuating between 37 to 40. We also observe improve in accuracy by nearly 2 percent.
```{r}
#set.seed(73)
#knn_acc <- train(Disease ~ ., data = train_dm_train, method = "knn",
#                    tuneGrid = data.frame(k = seq(30, 40)),
#                    trControl = trainControl(method = "repeatedcv",
#                                             number = 3, repeats = 3))
print(knn_acc)
```

##### Plotting the graph for accuracy of the model vs the number of neighbors selected in a model. It shows that there a linear increase in accuracy with increase in K value.
```{r}
plot(knn_acc)
```

##### Plotting the confusion matrix to evaluate model performance on validation dataset by analysing the evaluation parameters such as accuracy, recall, specificity etc. Important point to note: Very small difference between training and validation accuracy: So there is no overfitting, also there is remarkable __increase in Recall__ 
```{r}
knn_acc_predict <- predict(knn_acc, newdata = train_dm_validate)
confusionMatrix(knn_acc_predict, train_dm_validate$Disease)
```

##### Now, fine tuning the model using hyperparameters and using cross validation and running the model for __'recall'__. The reason for running the model for recall is that in this problem we want to reduce the __type 2 error__ i.e. we'll have to reduce the false negative values, hence increase the recall.
```{r}
#knn_sens <- train(make.names(Disease) ~ ., data = train_dm_train, method = "knn",
#                    tuneGrid = data.frame(k = seq(30, 40)),
#                    trControl = trainControl(method = "repeatedcv",
#                                             number = 3, repeats = 3, classProbs = TRUE,    #summaryFunction = prSummary), metric="Recall")
print(knn_sens)
```

##### We ran the model for both accuracy and sensitivity. For the model knn_sens we were getting slightly higher sensitivity, but when we ran the model on validation dataset, the model knn_acc had better accuracy but recall values were nearly same as those for knn_sens. Hence, for predicting the values for test dataset we will go ahead with the __knn_acc model__ (i.e. the model working for accuracy)

##### Plotting the graph for accuracy of the model vs the number of neighbors selected in a model.
```{r}
plot(knn_sens)
```

##### Plotting the confusion matrix to evaluate model performance on validation dataset by analysing the evaluation parameters such as accuracy, recall, specificity etc. 
```{r}
knn_sens_pred <- predict(knn_sens, newdata = train_dm_validate)
confusionMatrix(knn_sens_pred, as.factor(make.names(train_dm_validate$Disease)))
```

##### Creating a function to plot the Receiver Operating Characteristics curve. Using sensitivity (recall) on y-axis and (1 - specificity) on x-axis we plot this curve. Values are obtained by keeping probability for every point as a threshold.  
```{r}
ROC_AUC <- function(x){
  roc_prob_pred <- predict(x, newdata = train_dm_validate, type = 'prob')
  roc_curve=roc(response=train_dm_validate$Disease, predictor= roc_prob_pred$`1`)
  fig <- plot(roc_curve, col="red", lwd=3, main="ROC curve")
  score <- auc(roc_curve)
  ro_au <- list(fig, score)
  return(ro_au)
}
```

##### Plotting the ROC curve for predicted classes of an observation and Calculating he area under that curve (AUC)
```{r}
ROC_AUC(knn_acc)
```

### __b] Naive-Bayes Classifier__
##### __Naive-Bayes Classifier Base Model__
##### Running the naive bayes base model on training dataset. Naive Bayes provided in "caret" package and "klaR" was giving error for this dataset. So using the "naiveBayes" from __"e1071"__ library
##### This model used __bayes theorem__ to predict the probability of an observation belonging to a class based on a __naive assumption__ that all the 'X' variables in data are independent of each other. It combines the prior knowledge __[P(Y)]__ with evidence __[P(X)]__ with the help of likelihood probability __[p(Y|X)]__
##### Naive Bayes model is giving very high recall for the model. Accuracy stays in close range of KNN.
```{r}
#nb_base <- naiveBayes(Disease ~ ., data = train_dm_train)
print(nb_base)
summary(nb_base)
```

##### Plotting the confusion matrix to evaluate model performance on validation dataset by analysing the evaluation parameters such as accuracy, recall, specificity etc. 
```{r}
nb_base_predict <- predict(nb_base, train_dm_validate)
confusionMatrix(nb_base_predict, train_dm_validate$Disease)
```

### __Naive Bayes Classifier Tuned__
##### Naive bayes classifier calculates product of probabilities of features for a class. If the data dosen't have enough values, we might face a problem of combination of X variables which never occured for a class. This will give zero probability and this 0 will run through all the features to make probability for a class = 0. Hence, we add a __laplace smoother__, it adds a small non zero number to the numerator which ensures non zero probability for any class.
```{r}
#nb_models <- list()
#for  (i in 1:5){
#  nb_tuned <- naiveBayes(Disease ~ ., data = train_dm_train, laplace = i)
#  lap_par <- toString(i)
#	nb_models[[lap_par]] <- nb_tuned
#}
#Accuracies <- c()
#for (i in 1:length(nb_models)){
#  nb_find_best <- predict(nb_models[[i]], train_dm_validate)
#  cm_nb <- confusionMatrix(nb_find_best, train_dm_validate$Disease)
#  Accuracies <- c(Accuracies, cm_nb$overall["Accuracy"])
#}
#nb_tuned_best <- nb_models[[which.max(Accuracies)]]
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
```{r}
nb_tuned_best_predict <- predict(nb_tuned_best, train_dm_validate)
confusionMatrix(nb_tuned_best_predict, train_dm_validate$Disease)
```

##### Plotting the ROC curve for predicted classes of an observation and Calculating he area under that curve (AUC)
```{r}
nb_prob_pred <- predict(nb_tuned_best, newdata = train_dm_validate, type = 'raw')
roc_curve=roc(response=train_dm_validate$Disease, predictor= nb_prob_pred[,'2'])
plot(roc_curve, col="red", lwd=3, main="ROC curve")
auc(roc_curve)
```

### __c] Random Forest__
##### Running the baseline model of Random Forest for training dataset.
##### Random forest is made of multiple decision trees. Unlike boosting methods, it has __fixed probability distribution__. mtry features are randomly selected for each decision tree. *__Generalisation error__* is directly proportional to the *__correlation_* between decision trees but the inversely proportional to the *__strength__* of the tree. Hence, we have to select the best possible number for mtry, which we will do in the next model. 
```{r}
#model_rf <- train(Disease ~ ., data = train_dm_train, method = "rf")
model_rf
```

##### __Now running the random forest model by tuning it for hyperparameters. Here hyperparameters include:__
##### 1] ".mtry": that means number of columns selected randomly for each decision tree in random forest. Ideally, to calculate mtry this formula is used:
##### *__mtry = [log_to_the_base(ncol) + 1]__* After running, multiple models it was found that this is giving more generalisation error, hence reduced to other best number.
##### 2] "ntree": Number of trees in random forest. 
##### Note that here we are not using any hyperparameters for pruning decision tree. The reason for that is every tree in random forest has very limited number of columns. So, tree length is very short. Hence, trees in random forest are __preferred to grow to the full length__ as that will __reduce the bias__ in our predictions. ntree parameter cannot be specified in a grid and hence we will use __for loop__ to run through multiple ntree values.
```{r}
#mt <- round(sqrt(ncol(train_dm_train)))
#control <- trainControl(method="repeatedcv", number=3, repeats=3)
#tunegrid <- expand.grid(.mtry=c((mt-2):(mt)))

#rf_models <- list()
#for (ntree in c(350, 400, 500)) {
#	set.seed(73)
#	rf_tuned <- train(Disease~., data=train_dm_train, method="rf", #metric="Accuracy",tuneGrid=tunegrid,
#	             trControl=control, ntree=ntree, verbose = TRUE)
#	num_trees <- toString(ntree)
#	rf_models[[num_trees]] <- rf_tuned
#}
```

##### Extracting the best model from all the models stored in list rf_models based on the accuracy of the 21 models
```{r}
Accuracy_results <- summary(results)$statistics$Accuracy
best_model_Index <- ceiling(which.max(Accuracy_results)/ncol(Accuracy_results))
rf_tuned_best <- rf_models[best_model_Index][[1]]
rf_tuned_best
```

##### dotplot generated for all the models of random forest for comparison
```{r}
dotplot(results)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
```{r}
rf_tuned_predict <- predict(rf_tuned_best, newdata = train_dm_validate)
confusionMatrix(rf_tuned_predict, train_dm_validate$Disease)
```

##### Plotting the ROC curve for predicted classes of an observation and Calculating he area under that curve (AUC)
```{r}
ROC_AUC(rf_tuned_best)
```

##### Checking the variable importance for the random forest model built by tuning the hyperparameters: Important observation:
##### 1.__High blood pressure__ were highly correlated variables as we saw in the correlation plot. Here we can see that both the variables have very high importance.
##### 2.Newly created variables such as __BMI__ and __BP_Category__ are helping a lot in model building and prediction.
```{r}
varImp(rf_tuned)
```

### __Support Vector Machines__
### __d] Support Vector Machines - Linear Base Model__
##### SVM linear model is a __maximum margin classifier__ model. Given that a binary classification data can be classified with multiple decision boundries, SVM linear tries to find a boundary with the margin. So, unlike other classifiers SVM needs more than just a decision boundary so that it can reduce the __generalisation error__ on the testing datatset.
```{r}
#set.seed(73)
#svm_lin_base <- train(Disease ~ ., data = train_dm_train,
#                          method = "svmLinear")
print(svm_lin_base)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
## __Recall has crossed the value of 80%__
```{r}
svm_lin_base_predict <- predict(svm_lin_base, newdata = train_dm_validate)
confusionMatrix(svm_lin_base_predict, train_dm_validate$Disease)
```

### __e] Support Vector Machines -  Linear Tuned Model__
##### Tuning the linear base SVM model with hyperparameters. The hyperparameter we will be using here 'C': 'C' is also known as cost. Linear SVM can have __soft margin__ which means it can have flexible decision boundary. It can be done by adding __slack variable__ to the minimize function while creating the boundary. To __compensate for the slack variable__ we have to add this __'C'__ parameter to the cost function. 'C' allows the misclassification of some points so we might get boundary with large margin and help in reducing the generalization error. Higher 'C' value might result in __overfitting__.
```{r}
#set.seed(73)
#svm_lin_tuned <- train(make.names(Disease) ~ ., data = train_dm_train, method = "svmLinear",
#                 trControl = trainControl(method="repeatedcv", number=3, repeats=3), tuneGrid =                  expand.grid(C = seq(0.5, 0.7, 0.05)), metric="Accuracy")
print(svm_lin_tuned)
```

##### Plotting the curve for different values of C against the accuracy
```{r}
plot(svm_lin_tuned)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
## __Reducing type 2 error: we have recall > 80%__
```{r}
svm_lin_tuned_predict <- predict(svm_lin_tuned, newdata = train_dm_validate)
confusionMatrix(svm_lin_tuned_predict, as.factor(make.names(train_dm_validate$Disease)))
```

### SVM NonLinear Base 
##### Non-linear function helps in classification when a linear decision boundary is not able to classify the dataset. Mathematically, we should project the data into __higher dimensions__ and use the same linear boundary (hyperplane) to seperate the classes. These *__calculations become very complicated__* especially for higher dimension. Hence, SVM uses __kernel trick__ i.e. nothing but finding and applying a function that can perform the same calculations (measuring the similarity) in lower dimension only and hence __reducing the computation time__. 
```{r}
#svm_nl_base <- train(Disease ~ ., data = train_dm_train, method = "svmRadial")
print(svm_nl_base)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
##### Recall decreases but significant increase in accuracy.
```{r}
svm_nl_base_predict <- predict(svm_nl_base, newdata = train_dm_validate)
confusionMatrix(svm_nl_base_predict, train_dm_validate$Disease)
```

### SVM NonLinear Tuned
##### Non Linear SVM uses "svmRadial" function instead linear function for the calculation of the __similarity between landmarks and any given point__. For SVM, every other point is the landmark with respected to the given point. The similarity (distance) with every other observation is calculated for every observation. For this calculation svmRadial function is used. The hyperparameters we are providing are nothing but __'C'__ cost parameter same as before and __'Sigma'__ which is a parameter of radial function. Radial function is similar to *__'Guassian' distribution__* where:
##### *__lambda = 1/(2(sigma-squared))__* where lambda is a gaussian parameter
```{r}
#svm_nl_tuned <- train(Disease ~ ., data = train_dm_train,
#                       tuneGrid = expand.grid(sigma = seq(0.1, 0.3, 0.1), C = seq(0.5, 1.5, #0.5)),
#                       method = "svmRadial", trControl = trainControl(method = "repeatedcv",
#                       number = 3, repeats = 3))

print(svm_nl_tuned)
```

##### Plotting the graph for svm_nl_tuned model
```{r}
plot(svm_nl_tuned)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
```{r}
svm_nl_tuned_predict <- predict(svm_nl_tuned, newdata = train_dm_validate)
confusionMatrix(svm_nl_tuned_predict, train_dm_validate$Disease)
```

### __f] Gradient Boosting Machine__
##### __Gradient Boosting Machine - Base Model__
##### Just like random forest GBM model also works with multiple decision trees. Only diffrence is that in random forest all the trees run in parallel. __In GBM, trees run in series__. After every iterations the __weights__ for the samples (observations) are updated. The samples which were misclassified are given more weights that the one which were classified correctly. Adding to that __boosting works on sampling with replacement__. The samples which were not selected in the first iterations will also be given __higher weights__ in next iterations. In this way the model progresses and helps us in __reducing the bias__. 
```{r}
#gbm_base <- train(Disease ~ ., data = train_dm_train, method = "gbm")
print(gbm_base)
```

##### Plotting the graph for gbm_base model
```{r}
plot(gbm_base)
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
```{r}
gbm_base_predict <- predict(gbm_base, newdata = train_dm_validate)
confusionMatrix(gbm_base_predict, train_dm_validate$Disease)
```

### __GBM Tuned__
##### Now tuning the hyperparameters, we will be tuning model as a whole and individual trees as well:
##### 1]__interaction.depth__: this parameter specifies that how many splits we will perform on every decision tree in so that we will get the specified number of nodes in a decision tree. It is the maximum number of nodes in a decision tree. The value "6" is generally considered to be best for giving significant results.
##### 2] __n.tree__: This value tells the model how many decision trees to include in our entire model.
##### 3] __shrinkage__: This parameter defined the learning rate of gbm model. Just like L1 regularisations helps in reducing the unimportant features in regression, shrinkage parameter in gbm helps reduce the importance of useless decision trees. It reduced the speed of the model. "0.01" values is preferred for any datasets having 10,000+ observations.
##### 4] __n.minobsinnode__: minimum number of observations in the leaf nodes of the decision trees. 10 is considered to be good and big enough number. It should be less for smaller data with respect to rows.
```{r}
#tuneGrid_gbm <- expand.grid(interaction.depth=c(4,6,8), n.trees = c(400,500,600),
#                   shrinkage=c(0.01), n.minobsinnode=c(10))

#trainControl_gbm <- trainControl(method="cv", number=3)
#set.seed(28)
#gbm_tuned <- train(Disease ~ ., data = train_dm_train,
#                       tuneGrid = expand.grid(tuneGrid_gbm), method = "gbm",
#                       trControl = trainControl_gbm, verbose=FALSE, metric="Accuracy")
print(gbm_tuned)
```

##### Plotting the graph for gbm_tuned model. Accuracy increases with increase innumber of trees but considering the generalisation error we are limiting the ntree to 600.
```{r}
trellis.par.set(caretTheme())
plot(gbm_tuned)  
```

##### Now, predicting the values for validation dataset. Then using the predictions to plot confusion matrix. Evaluating the parameters such as accuracy and recall.
```{r}
gbm_tuned_predict <- predict(gbm_tuned, newdata = train_dm_validate)
confusionMatrix(gbm_tuned_predict, train_dm_validate$Disease)
```


##### Plotting the ROC curve for predicted classes of an observation and Calculating he area under that curve (AUC)
```{r}
ROC_AUC(gbm_tuned)
```

### Model Comparison
##### Comaparing all the models that we have built as of now: Those include, best models of KNN, naive bayes, Random forest, SVM linear, SVM non linear and gradient boosting.
##### Both Accuracy and Kappa values suggests that __Gradient boosting__ is the best algorithm (accuracy) for this problem
```{r}
model_comparison <- resamples(list(KNN_C = knn_base, RF = model_rf, SVML_C = svm_lin_base, SVMNL_C = svm_nl_base, GBM_C = gbm_base))
summary(model_comparison)
```

## Conclusion:
##### We hereby conclude that the problem dataset was ran through 6 different classification models. Powerful models like Supply vector machines and ensemble models (like gradient boosting) had advantage over less powerful models like naive bayes classifier and K-nearest neighbor. For various models the __accuracy__ varied between the range of __[70%, 75%]__ and the __recall__ (sensitivity) varied between the range of __[72%, 81%]__. 
### Out of all the models, __Supply vector machine with Radial function__ had the best balance between __accuracy[~74%]__ and __recall[80%]__. 


##### Transforming test data with respect to the transformed train data to pass it through the various models make our final predictions
```{r}
test_df <- read.csv('Disease Prediction Testing.csv', stringsAsFactors = F)

#apply(is.na(test_df),2,sum)

#num_test_df <- test_df[sapply(test_df, is.numeric) | sapply(test_df, is.integer)]
#quantiles_test <- c(0.001, 0.002, 0.003, 0.985, 0.99, 0.999, 1)
#out_test_df <- data.frame(row.names = quantiles_test)
#for (i in 1:length(num_test_df)){
#  r <- as.vector(quantile(num_test_df[,i], quantiles_test))
#  out_test_df[,i] <- r
#}
#colnames(out_test_df) <- colnames(num_test_df)
#print(out_test_df)

test_df$Low.Blood.Pressure <- Out_Treat_W(test_df$Low.Blood.Pressure, 0.002, 0.985)
test_df$High.Blood.Pressure <- Out_Treat_W(test_df$High.Blood.Pressure, 0.003, 0.999)

for (i in 1:nrow(test_df)) {
  if (test_df$High.Blood.Pressure[i] >= 180 | test_df$Low.Blood.Pressure[i] >= 120) {
    test_df$BP_Category[i] = "Hypertensive"
  }
  else if (test_df$High.Blood.Pressure[i] >= 140 | test_df$Low.Blood.Pressure[i] >= 90) {
    test_df$BP_Category[i] = "HT_stage2"
  }
  else if (test_df$High.Blood.Pressure[i] >= 130 | test_df$Low.Blood.Pressure[i] >= 80) {
    test_df$BP_Category[i] = "HT_stage1"
  }
  else {
    test_df$BP_Category[i] = "Normal"
  }
}
for (i in 1:nrow(test_df)) {
  test_df$BMI[i] <- (test_df$Weight[i] / ((test_df$Height[i]/100)**2))
}
lr_drop_test <- c('Height', 'Weight')
test_df <- test_df[,-which(names(test_df) %in% lr_drop_test)]

scale_test <- c('Age', 'High.Blood.Pressure', 'Low.Blood.Pressure', 'BMI')
for(i in 1:length(colnames(test_df))){
  if(colnames(test_df)[i] %in% scale_test){
    test_df[,i] <- as.vector(scale(test_df[,i]))
  }
}

dummies_test <- c('Gender','Cholesterol','Glucose', 'BP_Category')
test_df <- dummy_cols(test_df, select_columns = dummies_test)
test_df <- test_df[,-which(names(test_df) %in% dummies_test)]


flip_test <- c('Exercise', 'Alcohol', 'Smoke')
for(i in 1:length(colnames(test_df))){
  if(colnames(test_df)[i] %in% flip_test){
    for (j in 1:nrow(test_df)) {
      if(test_df[j,i] == 0){
        test_df[j,i] <- 1
      }
      else{
        test_df[j,i] <- 2
      }
    }
  }
}

test_df$ID <- NULL
```

##### Now predicting if a person will get the disease or will he/she be safe using actual test data set. Combining all the predictions into a dataframe and exporting the dataframe to a csv file
```{r}
NBC <- predict(nb_tuned_best, newdata = test_df)
KNN <- predict(knn_acc, newdata = test_df)
SVM_Linear <- predict(svm_lin_tuned, newdata = test_df)
SVM_RBF <- predict(svm_nl_tuned, newdata = test_df)
RF <- predict(rf_tuned_best, newdata = test_df)
GBM <- predict(gbm_tuned, newdata = test_df)

HW3_CSV <- data.frame(NBC, KNN, SVM_Linear, SVM_RBF, RF, GBM)

i <- sapply(HW3_CSV, is.factor)
HW3_CSV[i] <- lapply(HW3_CSV[i], as.integer)

HW3_CSV$NBC <- HW3_CSV$NBC - 1
HW3_CSV$KNN <- HW3_CSV$KNN - 1
HW3_CSV$SVM_Linear <- HW3_CSV$SVM_Linear - 1
HW3_CSV$SVM_RBF <- HW3_CSV$SVM_RBF - 1
HW3_CSV$RF <- HW3_CSV$RF - 1
HW3_CSV$GBM <- HW3_CSV$GBM - 1

#str(HW3_CSV)
#View(HW3_CSV)

#write.csv(HW3_CSV,"Assignment3.csv", row.names = TRUE)
```

